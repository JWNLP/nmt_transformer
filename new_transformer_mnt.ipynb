{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_transformer_mnt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JWNLP/nmt_transformer/blob/main/new_transformer_mnt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN1Bu_HRcTZC",
        "outputId": "f8e2729e-41e1-406f-e5d3-b41cb6affbd6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS72VFSGP8cR",
        "outputId": "099ccfd9-1077-4a04-8097-d7eeae9c6e6a"
      },
      "source": [
        "!pip install en-core-web-sm\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install soynlp\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en-core-web-sm in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (57.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en-core-web-sm) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en-core-web-sm) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en-core-web-sm) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en-core-web-sm) (3.7.4.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting soynlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G9rYcOUVfbZ",
        "outputId": "ac1be7f8-9ab3-462f-f6ea-138ef392b8e2"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install spacy download en_core_web_sm\n",
        "!pip install spacy download en"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting download\n",
            "  Downloading https://files.pythonhosted.org/packages/37/45/01e7455a9659528e77a414b222326d4c525796e4f571bbabcb2e0ff3d1f4/download-0.3.5-py3-none-any.whl\n",
            "Requirement already satisfied: en_core_web_sm in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from download) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Installing collected packages: download\n",
            "Successfully installed download-0.3.5\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: download in /usr/local/lib/python3.7/dist-packages (0.3.5)\n",
            "Collecting en\n",
            "  Downloading https://files.pythonhosted.org/packages/21/75/1ade7951baf8b99dc322714ab9dd80054d3cb9f668c27a778bf373d1c631/en-0.0.1.tar.gz\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from download) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Building wheels for collected packages: en\n",
            "  Building wheel for en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en: filename=en-0.0.1-cp37-none-any.whl size=1151 sha256=317352c41069b9350502eb1338b9a6586d0fb4379547bb7899e7d9745f12ae7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/43/90/056879336c6fad1a3c6b467a91f507232904a939224871653c\n",
            "Successfully built en\n",
            "Installing collected packages: en\n",
            "Successfully installed en-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3cvhT9uVcXp",
        "outputId": "29db847a-7604-4ed5-c1b5-d8c01dd35cdd"
      },
      "source": [
        "!pip install torch==1.9.0\n",
        "!pip install Torchtext==0.6.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n",
            "Collecting Torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from Torchtext==0.6.0) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Torchtext==0.6.0) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->Torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Torchtext==0.6.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Torchtext==0.6.0) (1.24.3)\n",
            "Installing collected packages: sentencepiece, Torchtext\n",
            "  Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed Torchtext-0.6.0 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn1yiNOVRDVS"
      },
      "source": [
        "## utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCpweFHzRBLv"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "from torchtext import data as ttd\n",
        "from torchtext.data import Example, Dataset\n",
        "\n",
        "\n",
        "def load_dataset(mode):\n",
        "    \"\"\"\n",
        "    Load train, valid and test dataset as a pandas DataFrame\n",
        "    Args:\n",
        "        mode: (string) configuration mode used to which dataset to load\n",
        "    Returns:\n",
        "        (DataFrame) train, valid, test dataset converted to pandas DataFrame\n",
        "    \"\"\"\n",
        "    print(f'Loading AI Hub Kor-Eng translation dataset and converting it to pandas DataFrame . . .')\n",
        "\n",
        "    data_dir =  '/content/drive/MyDrive/transformer/data/'\n",
        "\n",
        "    if mode == 'train':\n",
        "        train_file = os.path.join(data_dir, 'train.csv')\n",
        "        train_data = pd.read_csv(train_file, encoding='utf-8')\n",
        "\n",
        "        valid_file = os.path.join(data_dir, 'valid.csv')\n",
        "        valid_data = pd.read_csv(valid_file, encoding='utf-8')\n",
        "\n",
        "        print(f'Number of training examples: {len(train_data)}')\n",
        "        print(f'Number of validation examples: {len(valid_data)}')\n",
        "\n",
        "        return train_data, valid_data\n",
        "\n",
        "    else:\n",
        "        test_file = os.path.join(data_dir, 'test.csv')\n",
        "        test_data = pd.read_csv(test_file, encoding='utf-8')\n",
        "\n",
        "        print(f'Number of testing examples: {len(test_data)}')\n",
        "\n",
        "        return test_data\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    remove special characters from the input sentence to normalize it\n",
        "    Args:\n",
        "        text: (string) text string which may contain special character\n",
        "    Returns:\n",
        "        normalized sentence\n",
        "    \"\"\"\n",
        "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`…》]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_dataset(data, kor, eng):\n",
        "    \"\"\"\n",
        "    Pre-process input DataFrame and convert pandas DataFrame to torchtext Dataset.\n",
        "    Args:\n",
        "        data: (DataFrame) pandas DataFrame to be converted into torchtext Dataset\n",
        "        kor: torchtext Field containing Korean sentence\n",
        "        eng: torchtext Field containing English sentence\n",
        "    Returns:\n",
        "        (Dataset) torchtext Dataset containing 'kor' and 'eng' Fields\n",
        "    \"\"\"\n",
        "    # drop missing values not containing str value from DataFrame\n",
        "    missing_rows = [idx for idx, row in data.iterrows() if type(row.korean) != str or type(row.english) != str]\n",
        "    data = data.drop(missing_rows)\n",
        "\n",
        "    # convert each row of DataFrame to torchtext 'Example' containing 'kor' and 'eng' Fields\n",
        "    list_of_examples = [Example.fromlist(row.apply(lambda x: clean_text(x)).tolist(),\n",
        "                                         fields=[('kor', kor), ('eng', eng)]) for _, row in data.iterrows()]\n",
        "\n",
        "    # construct torchtext 'Dataset' using torchtext 'Example' list\n",
        "    dataset = Dataset(examples=list_of_examples, fields=[('kor', kor), ('eng', eng)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def make_iter(batch_size, mode, train_data=None, valid_data=None, test_data=None):\n",
        "    \"\"\"\n",
        "    Convert pandas DataFrame to torchtext Dataset and make iterator which will be used to train and test the model\n",
        "    Args:\n",
        "        batch_size: (integer) batch size used to make iterators\n",
        "        mode: (string) configuration mode used to which iterator to make\n",
        "        train_data: (DataFrame) pandas DataFrame used to build train iterator\n",
        "        valid_data: (DataFrame) pandas DataFrame used to build validation iterator\n",
        "        test_data: (DataFrame) pandas DataFrame used to build test iterator\n",
        "    Returns:\n",
        "        (BucketIterator) train, valid, test iterator\n",
        "    \"\"\"\n",
        "    # load text and label field made by build_pickles.py\n",
        "    file_kor = open('/content/drive/MyDrive/transformer/pickles/kor.pickle', 'rb')\n",
        "    kor = pickle.load(file_kor)\n",
        "\n",
        "    file_eng = open('/content/drive/MyDrive/transformer/pickles/eng.pickle', 'rb')\n",
        "    eng = pickle.load(file_eng)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # convert pandas DataFrame to torchtext dataset\n",
        "    if mode == 'train':\n",
        "        train_data = convert_to_dataset(train_data, kor, eng)\n",
        "        valid_data = convert_to_dataset(valid_data, kor, eng)\n",
        "\n",
        "        # make iterator using train and validation dataset\n",
        "        print(f'Make Iterators for training . . .')\n",
        "        train_iter, valid_iter = ttd.BucketIterator.splits(\n",
        "            (train_data, valid_data),\n",
        "            # the BucketIterator needs to be told what function it should use to group the data.\n",
        "            # In our case, we sort dataset using text of example\n",
        "            sort_key=lambda sent: len(sent.kor),\n",
        "            # all of the tensors will be sorted by their length by below option\n",
        "            sort_within_batch=True,\n",
        "            batch_size=batch_size,\n",
        "            device=device)\n",
        "\n",
        "        return train_iter, valid_iter\n",
        "\n",
        "    else:\n",
        "        test_data = convert_to_dataset(test_data, kor, eng)\n",
        "\n",
        "        # defines dummy list will be passed to the BucketIterator\n",
        "        dummy = list()\n",
        "\n",
        "        # make iterator using test dataset\n",
        "        print(f'Make Iterators for testing . . .')\n",
        "        test_iter, _ = ttd.BucketIterator.splits(\n",
        "            (test_data, dummy),\n",
        "            sort_key=lambda sent: len(sent.kor),\n",
        "            sort_within_batch=True,\n",
        "            batch_size=batch_size,\n",
        "            device=device)\n",
        "\n",
        "        return test_iter\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Calculate the time spent to train one epoch\n",
        "    Args:\n",
        "        start_time: (float) training start time\n",
        "        end_time: (float) training end time\n",
        "    Returns:\n",
        "        (int) elapsed_mins and elapsed_sec spent for one epoch\n",
        "    \"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def display_attention(candidate, translation, attention):\n",
        "    \"\"\"\n",
        "    displays the model's attention over the source sentence for each target token generated.\n",
        "    Args:\n",
        "        candidate: (list) tokenized source tokens\n",
        "        translation: (list) predicted target translation tokens\n",
        "        attention: a tensor containing attentions scores\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    # attention = [target length, source length]\n",
        "\n",
        "    attention = attention.cpu().detach().numpy()\n",
        "    # attention = [target length, source length]\n",
        "\n",
        "    font_location = 'pickles/NanumSquareR.ttf'\n",
        "    fontprop = fm.FontProperties(fname=font_location)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    ax.matshow(attention, cmap='bone')\n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels([''] + [t.lower() for t in candidate], rotation=45, fontproperties=fontprop)\n",
        "    ax.set_yticklabels([''] + translation, fontproperties=fontprop)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "class Params:\n",
        "    \"\"\"\n",
        "    Class that loads hyperparameters from a json file\n",
        "    Example:\n",
        "    ```\n",
        "    params = Params(json_path)\n",
        "    print(params.learning_rate)\n",
        "    params.learning_rate = 0.5  # change the value of learning_rate in params\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_path):\n",
        "        self.update(json_path)\n",
        "        self.load_vocab()\n",
        "\n",
        "    def update(self, json_path):\n",
        "        json_path ='/content/drive/MyDrive/transformer/data/params.json'\n",
        "        \"\"\"Loads parameters from json file\"\"\"\n",
        "        with open(json_path) as f:\n",
        "            params = json.load(f)\n",
        "            self.__dict__.update(params)\n",
        "\n",
        "    def load_vocab(self):\n",
        "        # load kor and eng vocabs to add vocab size configuration\n",
        "        pickle_kor = open('/content/drive/MyDrive/transformer/pickles/kor.pickle', 'rb')\n",
        "        kor = pickle.load(pickle_kor)\n",
        "\n",
        "        pickle_eng = open('/content/drive/MyDrive/transformer/pickles/eng.pickle', 'rb')\n",
        "        eng = pickle.load(pickle_eng)\n",
        "\n",
        "        # add device information to the the params\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # add <sos> and <eos> tokens' indices used to predict the target sentence\n",
        "        params = {'input_dim': len(kor.vocab), 'output_dim': len(eng.vocab),\n",
        "                  'sos_idx': eng.vocab.stoi['<sos>'], 'eos_idx': eng.vocab.stoi['<eos>'],\n",
        "                  'pad_idx': eng.vocab.stoi['<pad>'], 'device': device}\n",
        "\n",
        "        self.__dict__.update(params)\n",
        "\n",
        "    @property\n",
        "    def dict(self):\n",
        "        \"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']`\"\"\"\n",
        "        return self.__dict__"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF79A3EeQ3Pd"
      },
      "source": [
        "## build_pickles.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyr6PXQmP6Rr",
        "outputId": "ecebee00-76e8-46df-8db4-8ebdffb291e2"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse # 코랩에선 config = parser.parse_args(args=[])\n",
        "import easydict #코랩에선 argparse 대신 easydict\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "#from utils import convert_to_dataset\n",
        "\n",
        "from torchtext import data as ttd\n",
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "\n",
        "def build_tokenizer():\n",
        "    \"\"\"\n",
        "    Train soynlp tokenizer which will be used to tokenize Korean input sentence\n",
        "    \"\"\"\n",
        "    print(f'Now building soy-nlp tokenizer . . .')\n",
        "\n",
        "    data_dir = '/content/drive/MyDrive/transformer/data/'\n",
        "    train_file = os.path.join(data_dir, 'corpus.csv')\n",
        "\n",
        "    df = pd.read_csv(train_file, encoding='utf-8')\n",
        "\n",
        "    # if encounters non-text row, we should skip it\n",
        "    kor_lines = [row.korean\n",
        "                 for _, row in df.iterrows() if type(row.korean) == str]\n",
        "\n",
        "    word_extractor = WordExtractor(min_frequency=5)\n",
        "    word_extractor.train(kor_lines)\n",
        "\n",
        "    word_scores = word_extractor.extract()\n",
        "    cohesion_scores = {word: score.cohesion_forward\n",
        "                       for word, score in word_scores.items()}\n",
        "    \n",
        "    with open('/content/drive/MyDrive/transformer/pickles/tokenizer.pickle', 'wb') as pickle_out:\n",
        "        pickle.dump(cohesion_scores, pickle_out)\n",
        "\n",
        "\n",
        "def build_vocab(config):\n",
        "    \"\"\"\n",
        "    Build vocab used to convert input sentence into word indices using soynlp and spacy tokenizer\n",
        "    Args:\n",
        "        config: configuration containing various options\n",
        "    \"\"\"\n",
        "    pickle_tokenizer = open('/content/drive/MyDrive/transformer/pickles/tokenizer.pickle', 'rb')\n",
        "    cohesion_scores = pickle.load(pickle_tokenizer)\n",
        "    tokenizer = LTokenizer(scores=cohesion_scores)\n",
        "\n",
        "    # include lengths of the source sentences to use pack pad sequence\n",
        "    kor = ttd.Field(tokenize=tokenizer.tokenize,\n",
        "                    lower=True,\n",
        "                    batch_first=True)\n",
        "\n",
        "    eng = ttd.Field(tokenize='spacy',\n",
        "                    init_token='<sos>',\n",
        "                    eos_token='<eos>',\n",
        "                    lower=True,\n",
        "                    batch_first=True)\n",
        "\n",
        "    data_dir = '/content/drive/MyDrive/transformer/data/'\n",
        "    train_file = os.path.join(data_dir, 'train.csv')\n",
        "\n",
        "    # train_data = pd.read_csv(train_file, encoding='utf-8') 이 코드에서 ParserError가 발생하면 engine이랑 delimiter 상세히 기입하기\n",
        "    # ParserError: Error tokenizing data. C error: EOF inside string starting at row 27202 \n",
        "    train_data = pd.read_csv(train_file, encoding='utf-8',engine='python', delimiter=',' )\n",
        "    train_data = convert_to_dataset(train_data, kor, eng)\n",
        "\n",
        "    print(f'Build vocabulary using torchtext . . .')\n",
        "\n",
        "    kor.build_vocab(train_data, max_size=config.kor_vocab)\n",
        "    eng.build_vocab(train_data, max_size=config.eng_vocab)\n",
        "\n",
        "    print(f'Unique tokens in Korean vocabulary: {len(kor.vocab)}')\n",
        "    print(f'Unique tokens in English vocabulary: {len(eng.vocab)}')\n",
        "\n",
        "    print(f'Most commonly used Korean words are as follows:')\n",
        "    print(kor.vocab.freqs.most_common(20))\n",
        "\n",
        "    print(f'Most commonly used English words are as follows:')\n",
        "    print(eng.vocab.freqs.most_common(20))\n",
        "\n",
        "    with open('/content/drive/MyDrive/transformer/pickles/kor.pickle', 'wb') as kor_file:\n",
        "        pickle.dump(kor, kor_file)\n",
        "\n",
        "    with open('/content/drive/MyDrive/transformer/pickles/eng.pickle', 'wb') as eng_file:\n",
        "        pickle.dump(eng, eng_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Pickle Builder')\n",
        "\n",
        "    parser.add_argument('--kor_vocab', type=int, default=55000)\n",
        "    parser.add_argument('--eng_vocab', type=int, default=30000)\n",
        "\n",
        "    config = parser.parse_args(args=[])\n",
        "    #Jupyter notebook에서 argparse 이용할 때, 굳이 easydict으로 안바꾸고\n",
        "    # 마지막 config = parser.parse_args() 괄호안에 args=[] 추가\n",
        "\n",
        "    #parser = easydict.EasyDict({\n",
        "    #    \"kor_vocab\":55000,\n",
        "    #    \"eng_vocab\":30000\n",
        "    #})\n",
        "\n",
        "    #config = parser.parse_args(config=[])\n",
        "\n",
        "    \n",
        "\n",
        "    build_tokenizer()\n",
        "    build_vocab(config)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now building soy-nlp tokenizer . . .\n",
            "training was done. used memory 0.532 Gb\n",
            "all cohesion probabilities was computed. # words = 46586\n",
            "all branching entropies was computed # words = 112737\n",
            "all accessor variety was computed # words = 112737\n",
            "Build vocabulary using torchtext . . .\n",
            "Unique tokens in Korean vocabulary: 31035\n",
            "Unique tokens in English vocabulary: 11662\n",
            "Most commonly used Korean words are as follows:\n",
            "[('이', 3230), ('는', 2397), ('을', 2009), ('에', 1979), ('수', 1892), ('은', 1845), ('가', 1808), ('를', 1629), ('당신', 1617), ('해요', 1492), ('나는', 1461), ('요', 1443), ('의', 1367), ('우리', 1306), ('있어', 1277), ('할', 1238), ('것', 1225), ('서', 1204), ('한', 1082), ('있는', 1047)]\n",
            "Most commonly used English words are as follows:\n",
            "[('i', 13951), ('the', 13051), ('to', 9422), ('you', 8086), ('a', 6893), ('is', 5507), ('it', 5361), ('in', 3772), ('of', 3479), ('and', 3476), ('for', 3006), ('that', 2828), ('do', 2798), ('have', 2687), (\"'s\", 2665), ('we', 2602), ('my', 2502), ('this', 2434), (\"n't\", 2359), ('can', 2238)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gckZzGp8mpmu"
      },
      "source": [
        "##model/ops.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z82qPL0XmqMe"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "pickle_eng = open('/content/drive/MyDrive/transformer/pickles/eng.pickle', 'rb')\n",
        "eng = pickle.load(pickle_eng)\n",
        "pad_idx = eng.vocab.stoi['<pad>']\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def create_subsequent_mask(target):\n",
        "    \"\"\"\n",
        "    if target length is 5 and diagonal is 1, this function returns\n",
        "        [[0, 1, 1, 1, 1],\n",
        "         [0, 0, 1, 1, 1],\n",
        "         [0, 0, 0, 1, 1],\n",
        "         [0, 0, 0, 0, 1],\n",
        "         [0, 0, 0, 0, 0]]\n",
        "    :param target: [batch size, target length]\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    batch_size, target_length = target.size()\n",
        "\n",
        "    # torch.triu returns the upper triangular part of a matrix based on user defined diagonal\n",
        "    subsequent_mask = torch.triu(torch.ones(target_length, target_length), diagonal=1).bool().to(device)\n",
        "    # subsequent_mask = [target length, target length]\n",
        "\n",
        "    # repeat subsequent_mask 'batch size' times to cover all data instances in the batch\n",
        "    subsequent_mask = subsequent_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "    # subsequent_mask = [batch size, target length, target length]\n",
        "\n",
        "    return subsequent_mask\n",
        "\n",
        "\n",
        "def create_source_mask(source):\n",
        "    \"\"\"\n",
        "    create masking tensor for encoder's self attention\n",
        "    if sentence is [2, 193, 9, 27, 10003, 1, 1, 1, 3] and 2 denotes <sos>, 3 denotes <eos> and 1 denotes <pad>\n",
        "    masking tensor will be [False, False, False, False, False, True, True, True, False]\n",
        "    :param source: [batch size, source length]\n",
        "    :return: source mask\n",
        "    \"\"\"\n",
        "    source_length = source.shape[1]\n",
        "\n",
        "# 내가 변경한 코드 source_pad_idx \n",
        "    # create boolean tensors which will be used to mask padding tokens of both source and target sentence\n",
        "    source_pad_idx = (source == pad_idx)\n",
        "    # source_mask = [batch size, source length]\n",
        "\n",
        "    \n",
        "    ## create boolean tensors which will be used to mask padding tokens of both source and target sentence\n",
        "    #source_mask = (source == pad_idx)\n",
        "    ## source_mask = [batch size, source length]\n",
        "\n",
        "    # repeat sentence masking tensors 'sentence length' times\n",
        "    source_mask = source_pad_idx.unsqueeze(1).repeat(1, source_length, 1)\n",
        "    # source_mask = [batch size, source length, source length]\n",
        "\n",
        "    return source_mask\n",
        "\n",
        "\n",
        "def create_target_mask(source, target):\n",
        "    \"\"\"\n",
        "    create masking tensor for decoder's self attention and decoder's attention on the output of encoder\n",
        "    if sentence is [2, 193, 9, 27, 10003, 1, 1, 1, 3] and 2 denotes <sos>, 3 denotes <eos> and 1 denotes <pad>\n",
        "    masking tensor will be [False, False, False, False, False, True, True, True, False]\n",
        "    :param source: [batch size, source length]\n",
        "    :param target: [batch size, target length]\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    target_length = target.shape[1]\n",
        "\n",
        "    subsequent_mask = create_subsequent_mask(target)\n",
        "    # subsequent_mask = [batch size, target length, target length]\n",
        "\n",
        "# 내가 변경한 코드 source_pad_idx, target_pad_idx\n",
        "    source_pad_idx = (source == pad_idx)\n",
        "    target_pad_idx = (target == pad_idx)\n",
        "    # target_mask    = [batch size, target length]\n",
        "\n",
        "    # repeat sentence masking tensors 'sentence length' times\n",
        "    dec_enc_mask = source_pad_idx.unsqueeze(1).repeat(1, target_length, 1)\n",
        "    target_mask = target_pad_idx.unsqueeze(1).repeat(1, target_length, 1)\n",
        "\n",
        "\n",
        "    #source_mask = (source == pad_idx)\n",
        "    #target_mask = (target == pad_idx)\n",
        "    ## target_mask    = [batch size, target length]\n",
        "\n",
        "\n",
        "    # repeat sentence masking tensors 'sentence length' times\n",
        "    #dec_enc_mask = source_mask.unsqueeze(1).repeat(1, target_length, 1)\n",
        "    #target_mask = target_pad_idx.unsqueeze(1).repeat(1, target_length, 1)\n",
        "\n",
        "    # combine <pad> token masking tensor and subsequent masking tensor for decoder's self attention\n",
        "    target_mask = target_mask | subsequent_mask\n",
        "    # target_mask = [batch size, target length, target length]\n",
        "    return target_mask, dec_enc_mask\n",
        "\n",
        "\n",
        "def create_position_vector(sentence):\n",
        "    \"\"\"\n",
        "    create position vector which contains positional information\n",
        "    0th position is used for pad index\n",
        "    :param sentence: [batch size, sentence length]\n",
        "    :return: [batch size, sentence length]\n",
        "    \"\"\"\n",
        "    # sentence = [batch size, sentence length]\n",
        "    batch_size, _ = sentence.size()\n",
        "    pos_vec = np.array([(pos+1) if word != pad_idx else 0\n",
        "                        for row in range(batch_size) for pos, word in enumerate(sentence[row])])\n",
        "    pos_vec = pos_vec.reshape(batch_size, -1)\n",
        "    pos_vec = torch.LongTensor(pos_vec).to(device)\n",
        "    return pos_vec\n",
        "\n",
        "\n",
        "def create_positional_encoding(max_len, hidden_dim):\n",
        "    # PE(pos, 2i)     = sin(pos/10000 ** (2*i / hidden_dim))\n",
        "    # PE(pos, 2i + 1) = cos(pos/10000 ** (2*i / hidden_dim))\n",
        "    sinusoid_table = np.array([pos / np.power(10000, 2 * i / hidden_dim)\n",
        "                               for pos in range(max_len) for i in range(hidden_dim)])\n",
        "    # sinusoid_table = [max len * hidden dim]\n",
        "\n",
        "    sinusoid_table = sinusoid_table.reshape(max_len, -1)\n",
        "    # sinusoid_table = [max len, hidden dim]\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # calculate pe for even dimension\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # calculate pe for odd dimension\n",
        "\n",
        "    # convert numpy based sinusoid table to torch.tensor and repeat it 'batch size' times\n",
        "    sinusoid_table = torch.FloatTensor(sinusoid_table).to(device)\n",
        "    sinusoid_table[0] = 0.\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "def init_weight(layer):\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        "    if layer.bias is not None:\n",
        "        nn.init.constant_(layer.bias, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur6WIHxImc3i"
      },
      "source": [
        "##model/attention.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWjgJGDHltmu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#from model.ops import init_weight\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert params.hidden_dim % params.n_head == 0\n",
        "        self.attentions = nn.ModuleList([SelfAttention(params)\n",
        "                                         for _ in range(params.n_head)])\n",
        "        self.o_w = nn.Linear(params.hidden_dim, params.hidden_dim, bias=False)\n",
        "        init_weight(self.o_w)\n",
        "        self.dropout = nn.Dropout(params.dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query, key, value = [batch size, sentence length, hidden dim]\n",
        "\n",
        "        self_attentions = [attention(query, key, value, mask) for attention in self.attentions]\n",
        "        # self_attentions = [batch size, sentence length, attention dim] * num head\n",
        "        weighted_vs = [weighted_v[0] for weighted_v in self_attentions]\n",
        "        attentions = [weighted_v[1] for weighted_v in self_attentions]\n",
        "\n",
        "        weighted_v = torch.cat(weighted_vs, dim=-1)\n",
        "        # weighted_v = [batch size, sentence length, hidden dim]\n",
        "\n",
        "        output = self.dropout(self.o_w(weighted_v))\n",
        "        # output = [batch size, sentence length, hidden dim]\n",
        "\n",
        "        return output, attentions\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.hidden_dim = params.hidden_dim\n",
        "        self.attention_dim = params.hidden_dim // params.n_head\n",
        "\n",
        "        self.q_w = nn.Linear(self.hidden_dim, self.attention_dim, bias=False)\n",
        "        self.k_w = nn.Linear(self.hidden_dim, self.attention_dim, bias=False)\n",
        "        self.v_w = nn.Linear(self.hidden_dim, self.attention_dim, bias=False)\n",
        "        init_weight(self.q_w)\n",
        "        init_weight(self.k_w)\n",
        "        init_weight(self.v_w)\n",
        "\n",
        "        self.dropout = nn.Dropout(params.dropout)\n",
        "        self.scale_factor = torch.sqrt(torch.FloatTensor([self.attention_dim])).to(params.device)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query, key, value = [batch size, sentence length, hidden dim]\n",
        "\n",
        "        # create Q, K, V matrices using identical input sentence to calculate self-attention score\n",
        "        q = self.q_w(query)\n",
        "        k = self.k_w(key)\n",
        "        v = self.v_w(value)\n",
        "        # q, k, v = [batch size, sentence length, attention dim]\n",
        "\n",
        "        self_attention = torch.bmm(q, k.permute(0, 2, 1))\n",
        "        self_attention = self_attention / self.scale_factor\n",
        "        # self_attention = [batch size, sentence length, sentence length]\n",
        "\n",
        "        if mask is not None:\n",
        "            self_attention = self_attention.masked_fill(mask, -np.inf)\n",
        "\n",
        "        # normalize self attention score by applying soft max function on each row\n",
        "        attention_score = F.softmax(self_attention, dim=-1)\n",
        "        norm_attention_score = self.dropout(attention_score)\n",
        "        # attention_score = [batch size, sentence length, sentence length]\n",
        "\n",
        "        # compute \"weighted\" value matrix using self attention score and V matrix\n",
        "        weighted_v = torch.bmm(norm_attention_score, v)\n",
        "        # weighted_v = [batch size, sentence length, attention dim]\n",
        "\n",
        "        return self.dropout(weighted_v), attention_score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye4kQ0EXmbYx"
      },
      "source": [
        "##model/positionwise.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCC0ocMRahkE"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from model.ops import init_weight\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        # nn.Conv1d takes input whose size is (N, C): N is a batch size, C denotes a number of channels\n",
        "        self.conv1 = nn.Conv1d(params.hidden_dim, params.feed_forward_dim, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(params.feed_forward_dim, params.hidden_dim, kernel_size=1)\n",
        "        init_weight(self.conv1)\n",
        "        init_weight(self.conv2)\n",
        "        self.dropout = nn.Dropout(params.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, sentence length, hidden dim]\n",
        "\n",
        "        # permute x's indices to apply nn.Conv1d on input 'x'\n",
        "        x = x.permute(0, 2, 1)                        # x = [batch size, hidden dim, sentence length]\n",
        "        output = self.dropout(F.relu(self.conv1(x)))  # output = [batch size, feed forward dim, sentence length)\n",
        "        output = self.conv2(output)                   # output = [batch size, hidden dim, sentence length)\n",
        "\n",
        "        # permute again to restore output's original indices\n",
        "        output = output.permute(0, 2, 1)              # output = [batch size, sentence length, hidden dim]\n",
        "        return self.dropout(output)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT1MGvyWnTMg"
      },
      "source": [
        "##model/encoder.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6wDSB6ynTV4"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#from model.attention import MultiHeadAttention\n",
        "#from model.positionwise import PositionWiseFeedForward\n",
        "#from model.ops import create_positional_encoding, create_source_mask, create_position_vector\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(params.hidden_dim, eps=1e-6)\n",
        "        self.self_attention = MultiHeadAttention(params)\n",
        "        self.position_wise_ffn = PositionWiseFeedForward(params)\n",
        "\n",
        "    def forward(self, source, source_mask):\n",
        "        # source          = [batch size, source length, hidden dim]\n",
        "        # source_mask     = [batch size, source length, source length]\n",
        "\n",
        "        # Original Implementation: LayerNorm(x + SubLayer(x)) -> Updated Implementation: x + SubLayer(LayerNorm(x))\n",
        "        normalized_source = self.layer_norm(source)\n",
        "        output = source + self.self_attention(normalized_source, normalized_source, normalized_source, source_mask)[0]\n",
        "\n",
        "        normalized_output = self.layer_norm(output)\n",
        "        output = output + self.position_wise_ffn(normalized_output)\n",
        "        # output = [batch size, source length, hidden dim]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(params.input_dim, params.hidden_dim, padding_idx=params.pad_idx)\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0, std=params.hidden_dim**-0.5)\n",
        "        self.embedding_scale = params.hidden_dim ** 0.5\n",
        "        self.pos_embedding = nn.Embedding.from_pretrained(\n",
        "            create_positional_encoding(params.max_len+1, params.hidden_dim), freeze=True)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(params) for _ in range(params.n_layer)])\n",
        "        self.dropout = nn.Dropout(params.dropout)\n",
        "        self.layer_norm = nn.LayerNorm(params.hidden_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, source):\n",
        "        # source = [batch size, source length]\n",
        "        source_mask = create_source_mask(source)      # [batch size, source length, source length]\n",
        "        source_pos = create_position_vector(source)   # [batch size, source length]\n",
        "\n",
        "        source = self.token_embedding(source) * self.embedding_scale\n",
        "        source = self.dropout(source + self.pos_embedding(source_pos))\n",
        "        # source = [batch size, source length, hidden dim]\n",
        "\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            source = encoder_layer(source, source_mask)\n",
        "        # source = [batch size, source length, hidden dim]\n",
        "\n",
        "        return self.layer_norm(source)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPFj5G9qnsWf"
      },
      "source": [
        "##model/decoder.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuHzt5uinse2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from model.attention import MultiHeadAttention\n",
        "#from model.positionwise import PositionWiseFeedForward\n",
        "#from model.ops import create_positional_encoding, create_target_mask, create_position_vector\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(params.hidden_dim, eps=1e-6)\n",
        "        self.self_attention = MultiHeadAttention(params)\n",
        "        self.encoder_attention = MultiHeadAttention(params)\n",
        "        self.position_wise_ffn = PositionWiseFeedForward(params)\n",
        "\n",
        "    def forward(self, target, encoder_output, target_mask, dec_enc_mask):\n",
        "        # target          = [batch size, target length, hidden dim]\n",
        "        # encoder_output  = [batch size, source length, hidden dim]\n",
        "        # target_mask     = [batch size, target length, target length]\n",
        "        # dec_enc_mask    = [batch size, target length, source length]\n",
        "\n",
        "        # Original Implementation: LayerNorm(x + SubLayer(x)) -> Updated Implementation: x + SubLayer(LayerNorm(x))\n",
        "        norm_target = self.layer_norm(target)\n",
        "        output = target + self.self_attention(norm_target, norm_target, norm_target, target_mask)[0]\n",
        "\n",
        "        # In Decoder stack, query is the output from below layer and key & value are the output from the Encoder\n",
        "        norm_output = self.layer_norm(output)\n",
        "        sub_layer, attn_map = self.encoder_attention(norm_output, encoder_output, encoder_output, dec_enc_mask)\n",
        "        output = output + sub_layer\n",
        "\n",
        "        norm_output = self.layer_norm(output)\n",
        "        output = output + self.position_wise_ffn(norm_output)\n",
        "        # output = [batch size, target length, hidden dim]\n",
        "\n",
        "        return output, attn_map\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(params.output_dim, params.hidden_dim, padding_idx=params.pad_idx)\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0, std=params.hidden_dim**-0.5)\n",
        "        self.embedding_scale = params.hidden_dim ** 0.5\n",
        "        self.pos_embedding = nn.Embedding.from_pretrained(\n",
        "            create_positional_encoding(params.max_len+1, params.hidden_dim), freeze=True)\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(params) for _ in range(params.n_layer)])\n",
        "        self.dropout = nn.Dropout(params.dropout)\n",
        "        self.layer_norm = nn.LayerNorm(params.hidden_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, target, source, encoder_output):\n",
        "        # target              = [batch size, target length]\n",
        "        # source              = [batch size, source length]\n",
        "        # encoder_output      = [batch size, source length, hidden dim]\n",
        "        target_mask, dec_enc_mask = create_target_mask(source, target)\n",
        "        # target_mask / dec_enc_mask  = [batch size, target length, target/source length]\n",
        "        target_pos = create_position_vector(target)  # [batch size, target length]\n",
        "\n",
        "        target = self.token_embedding(target) * self.embedding_scale\n",
        "        target = self.dropout(target + self.pos_embedding(target_pos))\n",
        "        # target = [batch size, target length, hidden dim]\n",
        "\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            target, attention_map = decoder_layer(target, encoder_output, target_mask, dec_enc_mask)\n",
        "        # target = [batch size, target length, hidden dim]\n",
        "\n",
        "        target = self.layer_norm(target)\n",
        "        output = torch.matmul(target, self.token_embedding.weight.transpose(0, 1))\n",
        "        # output = [batch size, target length, output dim]\n",
        "        return output, attention_map"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAh0adj3n0je"
      },
      "source": [
        "##model/transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZmGBf_8n0pA"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#from model.encoder import Encoder\n",
        "#from model.decoder import Decoder\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(params)\n",
        "        self.decoder = Decoder(params)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # source = [batch size, source length]\n",
        "        # target = [batch size, target length]\n",
        "        encoder_output = self.encoder(source)                            # [batch size, source length, hidden dim]\n",
        "        output, attn_map = self.decoder(target, source, encoder_output)  # [batch size, target length, output dim]\n",
        "        return output, attn_map\n",
        "\n",
        "    def count_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REY-IAVoLBD"
      },
      "source": [
        "##model/optim.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwx_LOSGoO8r"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class ScheduledAdam():\n",
        "    def __init__(self, optimizer, hidden_dim, warm_steps):\n",
        "        self.init_lr = np.power(hidden_dim, -0.5)\n",
        "        self.optimizer = optimizer\n",
        "        self.current_steps = 0\n",
        "        self.warm_steps = warm_steps\n",
        "\n",
        "    def step(self):\n",
        "        # Update learning rate using current step information\n",
        "        self.current_steps += 1\n",
        "        lr = self.init_lr * self.get_scale()\n",
        "\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = lr\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.current_steps, -0.5),\n",
        "            self.current_steps * np.power(self.warm_steps, -0.5)\n",
        "        ])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvCDalaNMX3z"
      },
      "source": [
        "##bleu.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aNLg3lLMWXw"
      },
      "source": [
        "\"\"\"\n",
        "@author : Hyunwoong\n",
        "@when : 2019-12-22\n",
        "@homepage : https://github.com/gusdnd852\n",
        "\"\"\"\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def bleu_stats(hypothesis, reference):\n",
        "    \"\"\"Compute statistics for BLEU.\"\"\"\n",
        "    stats = []\n",
        "    stats.append(len(hypothesis))\n",
        "    stats.append(len(reference))\n",
        "    for n in range(1, 5):\n",
        "        s_ngrams = Counter(\n",
        "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
        "        )\n",
        "        r_ngrams = Counter(\n",
        "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
        "        )\n",
        "\n",
        "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
        "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def bleu(stats):\n",
        "    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n",
        "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
        "        return 0\n",
        "    (c, r) = stats[:2]\n",
        "    log_bleu_prec = sum(\n",
        "        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n",
        "    ) / 4.\n",
        "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
        "\n",
        "\n",
        "def get_bleu(hypotheses, reference):\n",
        "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
        "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    for hyp, ref in zip(hypotheses, reference):\n",
        "        stats += np.array(bleu_stats(hyp, ref))\n",
        "    return 100 * bleu(stats)\n",
        "\n",
        "\n",
        "def idx_to_word(x, vocab):\n",
        "    words = []\n",
        "    for i in x:\n",
        "        word = vocab.itos[i]\n",
        "        if '<' not in word:\n",
        "            words.append(word)\n",
        "    words = \" \".join(words)\n",
        "    return words"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkQ-OoUZpnGT"
      },
      "source": [
        "##trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Qm0JT1pnTW"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#from utils import epoch_time\n",
        "#from model.optim import ScheduledAdam\n",
        "#from model.transformer import Transformer\n",
        "\n",
        "random.seed(32)\n",
        "torch.manual_seed(32)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, params, mode, train_iter=None, valid_iter=None, test_iter=None):\n",
        "        self.params = params\n",
        "\n",
        "        # Train mode\n",
        "        if mode == 'train':\n",
        "            self.train_iter = train_iter\n",
        "            self.valid_iter = valid_iter\n",
        "\n",
        "        # Test mode\n",
        "        else:\n",
        "            self.test_iter = test_iter\n",
        "\n",
        "        self.model = Transformer(self.params)\n",
        "        self.model.to(self.params.device)\n",
        "\n",
        "        # Scheduling Optimzer\n",
        "        self.optimizer = ScheduledAdam(\n",
        "            optim.Adam(self.model.parameters(), betas=(0.9, 0.98), eps=1e-9),\n",
        "            hidden_dim=params.hidden_dim,\n",
        "            warm_steps=params.warm_steps\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.params.pad_idx)\n",
        "        self.criterion.to(self.params.device)\n",
        "\n",
        "    \n",
        "    #학습(training) 및 검증(validation) 진행\n",
        "    def train(self):\n",
        "        print(self.model)\n",
        "        print(f'The model has {self.model.count_params():,} trainable parameters')\n",
        "        best_valid_loss = float('inf')\n",
        "\n",
        "        for epoch in range(self.params.num_epoch):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0\n",
        "            start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "            # 전체 학습 데이터를 확인하며\n",
        "            for batch in self.train_iter:\n",
        "                # For each batch, first zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                source = batch.kor\n",
        "                target = batch.eng\n",
        "\n",
        "\n",
        "                # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "                # target sentence consists of <sos> and following tokens (except the <eos> token)\n",
        "                output = self.model(source, target[:, :-1])[0]\n",
        "\n",
        "                # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "                # ground truth sentence consists of tokens and <eos> token (except the <sos> token)\n",
        "                output = output.contiguous().view(-1, output.shape[-1])\n",
        "\n",
        "                # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "                target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                # output = [(batch size * target length - 1), output dim]\n",
        "                # target = [(batch size * target length - 1)]\n",
        "                \n",
        "                # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "                loss = self.criterion(output, target)\n",
        "                loss.backward()  # 기울기(gradient) 계산\n",
        "\n",
        "\n",
        "                # 기울기(gradient) clipping 진행\n",
        "                # clip the gradients to prevent the model from exploding gradient\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.params.clip)\n",
        "\n",
        "                # 파라미터 업데이트\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # 전체 손실 값 계산\n",
        "                # 'item' method is used to extract a scalar from a tensor which only contains a single value.\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            train_loss = epoch_loss / len(self.train_iter)\n",
        "            valid_loss = self.evaluate() \n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                # 모델 저장\n",
        "                torch.save(self.model.state_dict(), self.params.save_model) # \"save_model\": \"model.pt\",\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "            print(f'\\tVal. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):.3f}')\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval() #평가 모드\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        with torch.no_grad():\n",
        "            for batch in self.valid_iter:\n",
        "                source = batch.kor\n",
        "                target = batch.eng\n",
        "\n",
        "                # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "                output = self.model(source, target[:, :-1])[0]\n",
        "\n",
        "                # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "                output = output.contiguous().view(-1, output.shape[-1])\n",
        "\n",
        "                # output = [(batch size * target length - 1), output dim]\n",
        "                # target = [(batch size * target length - 1)]\n",
        "                \n",
        "                # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "                target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "                # 전체 손실 값 계산\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(self.valid_iter)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(torch.load(self.params.save_model))\n",
        "        self.model.eval()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.test_iter:\n",
        "                source = batch.kor\n",
        "                target = batch.eng\n",
        "\n",
        "                output = self.model(source, target[:, :-1])[0]\n",
        "\n",
        "                output = output.contiguous().view(-1, output.shape[-1])\n",
        "                target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        test_loss = epoch_loss / len(self.test_iter)\n",
        "        print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgRJKMaetyP0"
      },
      "source": [
        "##train_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxPYoKM8t1XS",
        "outputId": "63a33444-49c9-4920-888c-1c778978f58b"
      },
      "source": [
        "import csv\n",
        "import argparse\n",
        "#from trainer import Trainer\n",
        "#from utils import load_dataset, make_iter, Params\n",
        "\n",
        "\n",
        "def train_model(config):\n",
        "    params = Params('/content/drive/MyDrive/transformer/data/params.json')\n",
        "\n",
        "    if config.mode == 'train':\n",
        "        train_data, valid_data = load_dataset(config.mode)\n",
        "        train_iter, valid_iter = make_iter(params.batch_size, config.mode,\n",
        "                                           train_data=train_data, valid_data=valid_data)\n",
        "\n",
        "        trainer = Trainer(params, config.mode, train_iter=train_iter, valid_iter=valid_iter)\n",
        "        trainer.train()\n",
        "\n",
        "    else:\n",
        "        test_data = load_dataset(config.mode)\n",
        "        test_iter = make_iter(params.batch_size, config.mode, test_data=test_data)\n",
        "\n",
        "        trainer = Trainer(params, config.mode, test_iter=test_iter)\n",
        "        trainer.test()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Transformer Neural Machine Translation')\n",
        "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
        "    args = parser.parse_args(args=[])\n",
        "    main(args)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading AI Hub Kor-Eng translation dataset and converting it to pandas DataFrame . . .\n",
            "Number of training examples: 27201\n",
            "Number of validation examples: 11500\n",
            "Make Iterators for training . . .\n",
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (token_embedding): Embedding(31035, 512, padding_idx=1)\n",
            "    (pos_embedding): Embedding(65, 512)\n",
            "    (encoder_layers): ModuleList(\n",
            "      (0): EncoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): EncoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): EncoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): EncoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): EncoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): EncoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (token_embedding): Embedding(11662, 512, padding_idx=1)\n",
            "    (pos_embedding): Embedding(65, 512)\n",
            "    (decoder_layers): ModuleList(\n",
            "      (0): DecoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): DecoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): DecoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): DecoderLayer(\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder_attention): MultiHeadAttention(\n",
            "          (attentions): ModuleList(\n",
            "            (0): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): SelfAttention(\n",
            "              (q_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (k_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (v_w): Linear(in_features=512, out_features=64, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (o_w): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (position_wise_ffn): PositionWiseFeedForward(\n",
            "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
            "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "The model has 65,946,112 trainable parameters\n",
            "Epoch: 01 | Epoch Time: 1m 34s\n",
            "\tTrain Loss: 5.954 | Train PPL: 385.213\n",
            "\tVal. Loss: 5.367 | Val. PPL: 214.275\n",
            "Epoch: 02 | Epoch Time: 1m 36s\n",
            "\tTrain Loss: 5.214 | Train PPL: 183.771\n",
            "\tVal. Loss: 5.164 | Val. PPL: 174.805\n",
            "Epoch: 03 | Epoch Time: 1m 36s\n",
            "\tTrain Loss: 5.002 | Train PPL: 148.638\n",
            "\tVal. Loss: 5.076 | Val. PPL: 160.095\n",
            "Epoch: 04 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.858 | Train PPL: 128.762\n",
            "\tVal. Loss: 5.016 | Val. PPL: 150.787\n",
            "Epoch: 05 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.744 | Train PPL: 114.915\n",
            "\tVal. Loss: 4.978 | Val. PPL: 145.208\n",
            "Epoch: 06 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.650 | Train PPL: 104.582\n",
            "\tVal. Loss: 4.963 | Val. PPL: 142.974\n",
            "Epoch: 07 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.588 | Train PPL: 98.332\n",
            "\tVal. Loss: 4.940 | Val. PPL: 139.788\n",
            "Epoch: 08 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.525 | Train PPL: 92.309\n",
            "\tVal. Loss: 4.906 | Val. PPL: 135.061\n",
            "Epoch: 09 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.469 | Train PPL: 87.283\n",
            "\tVal. Loss: 4.882 | Val. PPL: 131.926\n",
            "Epoch: 10 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 4.423 | Train PPL: 83.331\n",
            "\tVal. Loss: 4.898 | Val. PPL: 133.995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-laFm7Y9o2d",
        "outputId": "a102b32a-4e8a-43ef-a4c3-962ca2877e40"
      },
      "source": [
        "import csv\n",
        "import argparse\n",
        "#from trainer import Trainer\n",
        "#from utils import load_dataset, make_iter, Params\n",
        "\n",
        "\n",
        "def test_model(config):\n",
        "    params = Params('/content/drive/MyDrive/transformer/data/test_params.json')\n",
        "    \n",
        "    if config.mode == 'test':\n",
        "        test_data = load_dataset(config.mode)\n",
        "        test_iter = make_iter(params.batch_size, config.mode, test_data=test_data)\n",
        "\n",
        "        trainer = Trainer(params, config.mode, test_iter=test_iter)\n",
        "        trainer.test()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Transformer Neural Machine Translation')\n",
        "    parser.add_argument('--mode', type=str, default='test', choices=['train', 'test'])\n",
        "    args = parser.parse_args(args=[])\n",
        "    main(args)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading AI Hub Kor-Eng translation dataset and converting it to pandas DataFrame . . .\n",
            "Number of testing examples: 11500\n",
            "Make Iterators for testing . . .\n",
            "Test Loss: 4.897 | Test PPL: 133.840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gWQ1Im4srxj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGvetw3wsr62"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jont7Mdf844M"
      },
      "source": [
        "##BLEU SCORE ERROR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-SudG23iv3F"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#from utils import epoch_time\n",
        "#from model.optim import ScheduledAdam\n",
        "#from model.transformer import Transformer\n",
        "\n",
        "random.seed(32)\n",
        "torch.manual_seed(32)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, params, mode, train_iter=None, valid_iter=None, test_iter=None):\n",
        "        self.params = params\n",
        "\n",
        "        # Train mode\n",
        "        if mode == 'train':\n",
        "            self.train_iter = train_iter\n",
        "            self.valid_iter = valid_iter\n",
        "\n",
        "        # Test mode\n",
        "        else:\n",
        "            self.test_iter = test_iter\n",
        "\n",
        "        self.model = Transformer(self.params)\n",
        "        self.model.to(self.params.device)\n",
        "\n",
        "        # Scheduling Optimzer\n",
        "        self.optimizer = ScheduledAdam(\n",
        "            optim.Adam(self.model.parameters(), betas=(0.9, 0.98), eps=1e-9),\n",
        "            hidden_dim=params.hidden_dim,\n",
        "            warm_steps=params.warm_steps\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.params.pad_idx)\n",
        "        self.criterion.to(self.params.device)\n",
        "\n",
        "    \n",
        "    #학습(training) 및 검증(validation) 진행\n",
        "    def train(self):\n",
        "        print(self.model)\n",
        "        print(f'The model has {self.model.count_params():,} trainable parameters')\n",
        "        best_valid_loss = float('inf')\n",
        "\n",
        "        for epoch in range(self.params.num_epoch):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0\n",
        "            start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "            # 전체 학습 데이터를 확인하며\n",
        "            for batch in self.train_iter:\n",
        "                # For each batch, first zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                source = batch.kor\n",
        "                target = batch.eng\n",
        "\n",
        "\n",
        "                # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "                # target sentence consists of <sos> and following tokens (except the <eos> token)\n",
        "                output = self.model(source, target[:, :-1])[0]\n",
        "\n",
        "                # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "                # ground truth sentence consists of tokens and <eos> token (except the <sos> token)\n",
        "                output = output.contiguous().view(-1, output.shape[-1])\n",
        "\n",
        "                # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "                target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                # output = [(batch size * target length - 1), output dim]\n",
        "                # target = [(batch size * target length - 1)]\n",
        "                \n",
        "                # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "                loss = self.criterion(output, target)\n",
        "                loss.backward()  # 기울기(gradient) 계산\n",
        "\n",
        "\n",
        "                # 기울기(gradient) clipping 진행\n",
        "                # clip the gradients to prevent the model from exploding gradient\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.params.clip)\n",
        "\n",
        "                # 파라미터 업데이트\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # 전체 손실 값 계산\n",
        "                # 'item' method is used to extract a scalar from a tensor which only contains a single value.\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            train_loss = epoch_loss / len(self.train_iter)\n",
        "            valid_loss = self.evaluate() \n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                # 모델 저장\n",
        "                torch.save(self.model.state_dict(), self.params.save_model) # \"save_model\": \"model.pt\",\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "            print(f'\\tVal. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):.3f}')\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval() #평가 모드\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        with torch.no_grad():\n",
        "            for batch in self.valid_iter:\n",
        "                source = batch.kor\n",
        "                target = batch.eng\n",
        "\n",
        "                # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "                output = self.model(source, target[:, :-1])[0]\n",
        "\n",
        "                # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "                output = output.contiguous().view(-1, output.shape[-1])\n",
        "\n",
        "                # output = [(batch size * target length - 1), output dim]\n",
        "                # target = [(batch size * target length - 1)]\n",
        "                \n",
        "                # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "                target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "                # 전체 손실 값 계산\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(self.valid_iter)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(torch.load(self.params.save_model))\n",
        "        self.model.eval()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_bleu = []  # bleu score\n",
        "            for batch in self.test_iter:\n",
        "                source = batch.kor\n",
        "                target = batch.eng\n",
        "\n",
        "                output = self.model(source, target[:, :-1])[0]\n",
        "\n",
        "                output = output.contiguous().view(-1, output.shape[-1])\n",
        "                target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                total_bleu = []\n",
        "                for j in range(self.params.batch_size):\n",
        "                  try:\n",
        "                    source_words = idx_to_word(source[j], kor.vocab)\n",
        "                    target_words = idx_to_word(target[j], eng.vocab)\n",
        "                    output_words = output[j].max(dim=1)[1]\n",
        "                    output_words = idx_to_word(output_words, eng.vocab)\n",
        "\n",
        "                    print('source :', source_words)\n",
        "                    print('target :', target_words)\n",
        "                    print('predicted :', output_words)\n",
        "                    print()\n",
        "                    bleu = get_bleu(hypotheses=output_words.split(), reference=target_words.split())\n",
        "                    total_bleu.append(bleu)\n",
        "                \n",
        "                  except:\n",
        "                    pass\n",
        "\n",
        "                total_bleu = sum(total_bleu) / len(total_bleu)\n",
        "                print('BLEU SCORE = {}'.format(total_bleu))\n",
        "                batch_bleu.append(total_bleu)\n",
        "\n",
        "            batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
        "            print(\"TOTAL BLEU SCORE = {}\".format(batch_bleu))\n",
        "\n",
        "\n",
        "        test_loss = epoch_loss / len(self.test_iter)\n",
        "        print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')\n",
        "        print(f'\\tBLEU Score: {bleu:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}